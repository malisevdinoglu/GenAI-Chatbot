import json
import os
from pathlib import Path

import streamlit as st

from create_vector_store import load_and_prepare_data, create_new_vector_store
# main.py dosyasÄ±ndan gerekli fonksiyonlarÄ± ve deÄŸiÅŸkeni import ediyoruz
from main import load_vector_store, create_conversational_chain, PROJECT_ID 

# --- GÃœNCELLENEN KISIM: Secrets'Ä± okuma ve Ortam DeÄŸiÅŸkeni olarak ayarlama ---
try:
    # Secrets'tan deÄŸiÅŸkenleri okuyup os.environ'a ayarlÄ±yoruz.
    project_id_secret = st.secrets["GOOGLE_PROJECT_ID"]
    service_account_secret = st.secrets["GOOGLE_SERVICE_ACCOUNT"]
    location_secret = st.secrets.get("GOOGLE_LOCATION", "us-central1")

    # Servis hesabÄ± JSON'unu geÃ§ici dosyaya yazÄ±p Vertex/AI Platform'a tanÄ±tÄ±yoruz.
    credentials_path = Path("/tmp/streamlit_service_account.json")
    if isinstance(service_account_secret, str):
        # Ã‡ok satÄ±rlÄ± JSON string'lerde kaÃ§Ä±ÅŸ karakterlerini gerÃ§ek satÄ±r sonlarÄ±na Ã§eviriyoruz.
        secret_text = service_account_secret.strip().replace("\\n", "\n")
        credentials_path.write_text(secret_text)
    else:
        credentials_path.write_text(json.dumps(dict(service_account_secret)))

    os.environ["GOOGLE_PROJECT_ID"] = project_id_secret
    os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = str(credentials_path)
    os.environ["GOOGLE_LOCATION"] = location_secret

    PROJECT_ID = project_id_secret

    st.session_state["location"] = location_secret  # Streamlit baÅŸlÄ±ÄŸÄ± iÃ§in konumu sakla
except Exception as exc:
    st.warning("Google Cloud secrets yÃ¼klenemedi. Deploy ortamÄ± iÃ§in Streamlit secrets'Ä± kontrol edin.")
    print(f"Secrets yÃ¼klenirken hata: {exc}")
# ---------------------------------------------------------------------------------


# Streamlit, bu fonksiyonu sadece bir kere Ã§alÄ±ÅŸtÄ±rÄ±r ve sonucunu Ã¶nbelleÄŸe alÄ±r.

@st.cache_resource
def setup_rag_pipeline():
    """RAG zincirini yÃ¼kler ve hazÄ±rlar, yoksa oluÅŸturur."""
    
    # ChromaDB klasÃ¶r adÄ±nÄ± tanÄ±mla
    CHROMA_DB_PATH = "chroma_db_recipes" 
    
    if not PROJECT_ID:
        st.error("HATA: Proje ID'si ayarlanmadÄ±. LÃ¼tfen Streamlit Secrets'Ä± kontrol edin.")
        return None

    # 1. VektÃ¶r deposu yÃ¼kleniyor
    st.write("VektÃ¶r deposu yÃ¼kleniyor...")
    # Chroma'yÄ± yÃ¼klemek iÃ§in doÄŸru klasÃ¶r adÄ±nÄ± main.py'deki fonksiyona iletiyoruz.
    vector_store = load_vector_store(project_id=PROJECT_ID, index_path=CHROMA_DB_PATH)
    
    # 2. EÄŸer yÃ¼klenemezse (klasÃ¶r yoksa), sÄ±fÄ±rdan oluÅŸturmayÄ± dene (OTOMATÄ°K OLUÅTURMA)
    if not vector_store:
        # SarÄ± uyarÄ±yÄ± gÃ¶sterir (FAISS/Chroma'nÄ±n oluÅŸturulduÄŸu an)
        st.warning("VeritabanÄ± bulunamadÄ±. VeritabanÄ± yeniden oluÅŸturuluyor. Bu iÅŸlem birkaÃ§ dakika sÃ¼rebilir...")
        
        # Veri setini yÃ¼kle
        recipe_docs = load_and_prepare_data('recipes.csv') 
        
        if recipe_docs:
            # VektÃ¶r veritabanÄ±nÄ± oluÅŸtur ve kaydet
            create_new_vector_store(recipe_docs, PROJECT_ID)
            
            # OluÅŸturulduktan sonra tekrar yÃ¼klemeyi dene
            vector_store = load_vector_store(project_id=PROJECT_ID, index_path=CHROMA_DB_PATH)
            
            if not vector_store:
                 st.error("VeritabanÄ± oluÅŸturulduktan sonra bile yÃ¼klenemedi. LÃ¼tfen loglarÄ± kontrol edin.")
                 return None
        else:
             # recipes.csv bulunamazsa bu hatayÄ± verir.
             st.error("Veri (recipes.csv) yÃ¼klenemediÄŸi iÃ§in veritabanÄ± oluÅŸturulamadÄ±.")
             return None

    # 3. Sohbet zinciri oluÅŸturuluyor
    st.write("Sohbet zinciri oluÅŸturuluyor...")
    chain = create_conversational_chain(project_id=PROJECT_ID, vector_store=vector_store)
    
    if not chain:
        st.error("Sohbet zinciri oluÅŸturulurken bir hata oluÅŸtu.")
        return None
        
    return chain

# --- Streamlit ArayÃ¼zÃ¼ ---

st.set_page_config(page_title="Tarif AsistanÄ±", layout="wide")
st.title("ğŸœ SaÄŸlÄ±klÄ± Tarif AsistanÄ± (RAG Chatbot)")
# BaÅŸlÄ±kta location bilgisini de gÃ¶sterelim
st.caption(f"LLM: Gemini / Embedding: text-embedding-004 / Konum: {st.session_state.get('location', 'us-central1')}")

# RAG zincirini kur
rag_chain = setup_rag_pipeline()

if rag_chain:
    
    # Sohbet geÃ§miÅŸini Streamlit session state'te tut
    if "messages" not in st.session_state:
        st.session_state.messages = []
        # Ä°lk karÅŸÄ±lama mesajÄ±
        st.session_state.messages.append({"role": "assistant", "content": "Merhaba! Ben sizin tarif asistanÄ±nÄ±zÄ±m. NasÄ±l bir tarif arÄ±yorsunuz?"})

    # GeÃ§miÅŸ mesajlarÄ± gÃ¶rÃ¼ntÃ¼le
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # KullanÄ±cÄ±dan girdi al
    if prompt := st.chat_input("Tarif sorunuzu buraya yazÄ±n..."):
        
        # KullanÄ±cÄ± mesajÄ±nÄ± geÃ§miÅŸe ekle ve gÃ¶ster
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # AsistanÄ±n cevabÄ±nÄ± al
        with st.chat_message("assistant"):
            with st.spinner("Tarif aranÄ±yor..."):
                try:
                    # LangChain zincirini Ã§aÄŸÄ±rÄ±yoruz
                    response = rag_chain.invoke({'question': prompt})
                    full_answer = response['answer']
                except Exception as e:
                    # Hata olursa logla ve kullanÄ±cÄ±ya gÃ¶ster
                    print(f"Zincir Ã§aÄŸrÄ±lÄ±rken hata: {e}")
                    full_answer = "ÃœzgÃ¼nÃ¼m, API Ã§aÄŸrÄ±sÄ±nda bir sorun oluÅŸtu. LÃ¼tfen uygulamanÄ±n loglarÄ±nÄ± kontrol edin."
                
                st.markdown(full_answer)
                
        # Asistan mesajÄ±nÄ± geÃ§miÅŸe ekle
        st.session_state.messages.append({"role": "assistant", "content": full_answer})
