import json
import os
from pathlib import Path

import streamlit as st
from google.oauth2 import service_account
import vertexai

# ChromaDB ve doÄŸru embedding importlarÄ±
from langchain_community.vectorstores import Chroma
from langchain_google_vertexai import VertexAIEmbeddings, VertexAI

# DiÄŸer LangChain importlarÄ±
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain_core.documents import Document # load_and_prepare_data iÃ§in gerekli

# create_vector_store.py'dan gerekli fonksiyonlarÄ± import ediyoruz
# (Bunlar zaten ChromaDB kullanÄ±yor olmalÄ±)
from create_vector_store import load_and_prepare_data, create_new_vector_store 

# Proje KimliÄŸiniz ve VarsayÄ±lan Model AyarlarÄ±
PROJECT_ID = "genai-final-project-475415" # Kendi Proje ID'nizle deÄŸiÅŸtirin
DEFAULT_EMBEDDING_MODEL = os.environ.get("VERTEX_EMBEDDING_MODEL", "text-embedding-005")
DEFAULT_LLM_MODEL = os.environ.get("VERTEX_LLM_MODEL", "gemini-1.5-flash")

def _get_streamlit_secret(key):
    """Streamlit secrets'tan gÃ¼venli ÅŸekilde okur; yoksa None dÃ¶ner."""
    if not hasattr(st, "secrets"):
        return None
    try:
        return st.secrets[key]
    except Exception:
        return None

def configure_google_credentials():
    """Render veya Streamlit gibi ortamlar iÃ§in kimlik bilgilerini hazÄ±rlar."""
    project_id = os.environ.get("GOOGLE_PROJECT_ID", PROJECT_ID) # DoÄŸrudan PROJECT_ID kullan
    location = os.environ.get("GOOGLE_LOCATION", "us-central1") # VarsayÄ±lan konumu ayarlayalÄ±m
    service_account_payload = (
        os.environ.get("GOOGLE_SERVICE_ACCOUNT_JSON")
        or os.environ.get("GOOGLE_SERVICE_ACCOUNT")
    )

    secret_project_id = _get_streamlit_secret("GOOGLE_PROJECT_ID")
    secret_location = _get_streamlit_secret("GOOGLE_LOCATION")
    secret_service_account = _get_streamlit_secret("GOOGLE_SERVICE_ACCOUNT")

    if secret_project_id:
        project_id = project_id or secret_project_id
    if secret_location:
        location = os.environ.get("GOOGLE_LOCATION", secret_location)
    if secret_service_account and not service_account_payload:
        secret_value = secret_service_account
        if isinstance(secret_value, str):
            service_account_payload = secret_value
        else:
            service_account_payload = json.dumps(dict(secret_value))

    # Lokal Ã§alÄ±ÅŸtÄ±rma iÃ§in gcloud ADC kullan (Payload yoksa)
    credentials = None
    if service_account_payload:
        try:
            if isinstance(service_account_payload, str):
                service_account_info = json.loads(service_account_payload.strip())
            else:
                service_account_info = dict(service_account_payload)
            
            if not service_account_info:
                 raise ValueError("Servis hesabÄ± bilgisi boÅŸ.")
            if not isinstance(service_account_info, dict):
                 service_account_info = dict(service_account_payload)

            credentials = service_account.Credentials.from_service_account_info(service_account_info)
            # GeÃ§ici dosya oluÅŸturmaya gerek yok, doÄŸrudan credentials kullanÄ±labilir
            
            # Ortam deÄŸiÅŸkenlerini yine de ayarlayalÄ±m (bazÄ± kÃ¼tÃ¼phaneler iÃ§in gerekebilir)
            os.environ["GOOGLE_PROJECT_ID"] = project_id
            # GOOGLE_APPLICATION_CREDENTIALS yerine doÄŸrudan kimlik bilgisi kullanacaÄŸÄ±z
            
        except Exception as exc:
            st.error(f"Google servis hesabÄ± JSON'u Ã§Ã¶zÃ¼mlenemedi: {exc}")
            print(f"Service account parse hatasÄ±: {exc}")
            return None, None, None
    else:
        # EÄŸer payload yoksa, gcloud ADC'nin ayarlÄ± olduÄŸunu varsayÄ±yoruz
        print("Servis hesabÄ± JSON'u bulunamadÄ±. Uygulama VarsayÄ±lan Kimlik Bilgileri (ADC) kullanÄ±lacak.")
        # Bu durumda credentials None kalÄ±r, kÃ¼tÃ¼phaneler ADC'yi kullanÄ±r.

    os.environ["GOOGLE_LOCATION"] = location # Konumu ayarlayalÄ±m
    
    # VertexAI init'e gerek yok, kÃ¼tÃ¼phaneler doÄŸrudan credentials veya ADC kullanÄ±r
    st.session_state["location"] = location
    st.session_state["project_id"] = project_id # Proje ID'sini session state'e ekle
    st.session_state["embedding_model"] = DEFAULT_EMBEDDING_MODEL
    st.session_state["llm_model"] = DEFAULT_LLM_MODEL
    st.session_state["credentials_provided"] = bool(credentials)

    return project_id, location, credentials


# Uygulama baÅŸÄ±nda kimlik bilgilerini yapÄ±landÄ±r
PROJECT_ID, LOCATION, _CREDENTIALS = configure_google_credentials()


# --- VektÃ¶r Deposu ve Zincir Kurulum FonksiyonlarÄ± (ChromaDB iÃ§in gÃ¼ncellendi) ---

def _get_vertex_credentials():
    """Service account varsa dÃ¶ner, aksi halde None (ADC)."""
    return _CREDENTIALS

def build_embeddings(project_id, location=None, model_name=DEFAULT_EMBEDDING_MODEL): # Model adÄ±nÄ± kontrol et
    """Vertex AI metin embedding modelini hazÄ±rlar."""
    # Kimlik bilgileri configure_google_credentials'dan alÄ±nacak (ADC veya service account)
    return VertexAIEmbeddings(
        project=project_id,
        location=location or LOCATION, # Global LOCATION kullan
        model_name=model_name,
        credentials=_get_vertex_credentials(),
    )

# @st.cache_resource kaldÄ±rÄ±ldÄ±, Ã§Ã¼nkÃ¼ load_vector_store artÄ±k oluÅŸturma da yapabilir
def load_or_create_vector_store(project_id, persist_directory="chroma_db_recipes"):
    """Mevcut ChromaDB deposunu yÃ¼kler, yoksa oluÅŸturur."""
    if not project_id:
         st.error("Proje ID'si yÃ¼klenemedi.")
         return None
         
    embeddings = build_embeddings(project_id=project_id)
    
    if os.path.exists(persist_directory):
        try:
            st.write(f"Mevcut vektÃ¶r deposu '{persist_directory}' yÃ¼kleniyor...")
            vector_store = Chroma(
                persist_directory=persist_directory,
                embedding_function=embeddings
            )
            print("ChromaDB baÅŸarÄ±yla yÃ¼klendi.")
            return vector_store
        except Exception as e:
            st.warning(f"Mevcut veritabanÄ± yÃ¼klenemedi, yeniden oluÅŸturulacak. Hata: {e}")
            # Hata durumunda yeniden oluÅŸturmaya devam et
    
    # EÄŸer klasÃ¶r yoksa veya yÃ¼klenemediyse, sÄ±fÄ±rdan oluÅŸtur
    st.warning(f"'{persist_directory}' bulunamadÄ± veya yÃ¼klenemedi. VeritabanÄ± oluÅŸturuluyor. Bu iÅŸlem birkaÃ§ dakika sÃ¼rebilir...")
    recipe_docs = load_and_prepare_data('recipes.csv', sample_size=200) # sample_size'Ä± ayarla
    
    if recipe_docs:
        try:
            vector_store = Chroma.from_documents(
                documents=recipe_docs, 
                embedding=embeddings,
                persist_directory=persist_directory
            )
            vector_store.persist() # Kaydetmeyi unutma!
            st.success(f"'{persist_directory}' baÅŸarÄ±yla oluÅŸturuldu.")
            print(f"'{persist_directory}' baÅŸarÄ±yla oluÅŸturuldu.")
            return vector_store
        except Exception as e:
            st.error(f"VeritabanÄ± oluÅŸturulurken hata oluÅŸtu: {e}")
            print(f"VeritabanÄ± oluÅŸturulurken hata oluÅŸtu: {e}")
            return None
    else:
         st.error("Veri (recipes.csv) yÃ¼klenemediÄŸi iÃ§in veritabanÄ± oluÅŸturulamadÄ±.")
         print("Veri (recipes.csv) yÃ¼klenemediÄŸi iÃ§in veritabanÄ± oluÅŸturulamadÄ±.")
         return None


# @st.cache_resource kaldÄ±rÄ±ldÄ±
def setup_conversational_chain(project_id, vector_store):
    """Sohbet zincirini oluÅŸturur."""
    if not project_id or not vector_store:
        return None
    try:
        st.write("Sohbet zinciri oluÅŸturuluyor...")
        llm = VertexAI(
            project=project_id,
            model_name=DEFAULT_LLM_MODEL,
            temperature=0.7,
            location=LOCATION,
            credentials=_get_vertex_credentials(),
        )
        
        memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True, output_key='answer') # output_key eklendi
        
        chain = ConversationalRetrievalChain.from_llm(
            llm=llm,
            retriever=vector_store.as_retriever(),
            memory=memory,
            return_source_documents=True # Kaynak dokÃ¼manlarÄ± da dÃ¶ndÃ¼r (opsiyonel)
        )
        print("Sohbet zinciri baÅŸarÄ±yla oluÅŸturuldu.")
        return chain
    except Exception as e:
        st.error(f"Sohbet zinciri oluÅŸturulurken bir hata oluÅŸtu: {e}")
        print(f"Sohbet zinciri oluÅŸturulurken bir hata oluÅŸtu: {e}")
        return None

# --- Streamlit ArayÃ¼zÃ¼ ---

st.set_page_config(page_title="Tarif AsistanÄ±", layout="wide")
st.title("ğŸœ SaÄŸlÄ±klÄ± Tarif AsistanÄ± (RAG Chatbot)")
st.caption(
    f"LLM: {st.session_state.get('llm_model', DEFAULT_LLM_MODEL)} "
    f"/ Embedding: {st.session_state.get('embedding_model', DEFAULT_EMBEDDING_MODEL)} "
    f"/ Konum: {st.session_state.get('location', 'Bilinmiyor')}"
)

# VektÃ¶r deposunu ve RAG zincirini kur (state iÃ§inde sakla)
if "rag_chain" not in st.session_state:
    st.session_state.vector_store = load_or_create_vector_store(project_id=PROJECT_ID)
    if st.session_state.vector_store:
        st.session_state.rag_chain = setup_conversational_chain(project_id=PROJECT_ID, vector_store=st.session_state.vector_store)
    else:
        st.session_state.rag_chain = None

# Zincir baÅŸarÄ±yla kurulduysa devam et
if st.session_state.rag_chain:
    
    # Sohbet geÃ§miÅŸini Streamlit session state'te tut
    if "messages" not in st.session_state:
        st.session_state.messages = []
        # Ä°lk karÅŸÄ±lama mesajÄ±
        st.session_state.messages.append({"role": "assistant", "content": "Merhaba! Ben sizin tarif asistanÄ±nÄ±zÄ±m. NasÄ±l bir tarif arÄ±yorsunuz?"})

    # GeÃ§miÅŸ mesajlarÄ± gÃ¶rÃ¼ntÃ¼le
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # KullanÄ±cÄ±dan girdi al
    if prompt := st.chat_input("Tarif sorunuzu buraya yazÄ±n..."):
        
        # KullanÄ±cÄ± mesajÄ±nÄ± geÃ§miÅŸe ekle ve gÃ¶ster
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # AsistanÄ±n cevabÄ±nÄ± al
        with st.chat_message("assistant"):
            with st.spinner("Tarif aranÄ±yor..."):
                try:
                    # LangChain zincirini Ã§aÄŸÄ±rÄ±yoruz
                    response = st.session_state.rag_chain.invoke({'question': prompt})
                    full_answer = response.get('answer', "ÃœzgÃ¼nÃ¼m, bir cevap alamadÄ±m.") 
                    # KaynaklarÄ± gÃ¶stermek isterseniz:
                    # source_docs = response.get('source_documents', [])
                    # if source_docs:
                    #     full_answer += "\n\n**Kaynaklar:**\n"
                    #     for doc in source_docs:
                    #          full_answer += f"- {doc.metadata.get('recipe_name', 'Bilinmeyen Tarif')}\n"
                             
                except Exception as e:
                    # Hata olursa logla ve kullanÄ±cÄ±ya gÃ¶ster
                    print(f"Zincir Ã§aÄŸrÄ±lÄ±rken hata: {e}")
                    st.error(f"Bir hata oluÅŸtu: {e}")
                    full_answer = "ÃœzgÃ¼nÃ¼m, API Ã§aÄŸrÄ±sÄ±nda bir sorun oluÅŸtu. LÃ¼tfen uygulamanÄ±n loglarÄ±nÄ± kontrol edin."
                
                st.markdown(full_answer)
                
        # Asistan mesajÄ±nÄ± geÃ§miÅŸe ekle
        st.session_state.messages.append({"role": "assistant", "content": full_answer})

elif PROJECT_ID: # EÄŸer zincir kurulamadÄ±ysa ama Proje ID varsa
    st.error("Uygulama baÅŸlatÄ±lamadÄ±. LÃ¼tfen loglarÄ± kontrol edin veya veritabanÄ±nÄ±n doÄŸru oluÅŸturulduÄŸundan emin olun.")
