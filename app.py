import streamlit as st
import os

from create_vector_store import load_and_prepare_data, create_new_vector_store
# main.py dosyasÄ±ndan gerekli fonksiyonlarÄ± ve deÄŸiÅŸkeni import ediyoruz
# Not: main.py dosyanÄ±zÄ±n aynÄ± klasÃ¶rde olduÄŸundan emin olun!
from main import load_vector_store, create_conversational_chain, PROJECT_ID 

# --- GÃœNCELLENEN KISIM: Secrets'Ä± okuma ve Ortam DeÄŸiÅŸkeni olarak ayarlama ---
try:
    # Secrets'tan deÄŸiÅŸkenleri okuyup os.environ'a ayarlÄ±yoruz.
    # Vertex AI (LangChain) bu ortam deÄŸiÅŸkenlerini otomatik olarak okuyacaktÄ±r.
    os.environ["GOOGLE_PROJECT_ID"] = st.secrets["GOOGLE_PROJECT_ID"] 
    os.environ["GOOGLE_SERVICE_ACCOUNT"] = st.secrets["GOOGLE_SERVICE_ACCOUNT"]

    # main.py'deki global PROJECT_ID deÄŸiÅŸkenini secrets'tan alÄ±nan deÄŸerle gÃ¼ncelliyoruz.
    # Bu, setup_rag_pipeline fonksiyonunun doÄŸru ID'yi kullanmasÄ±nÄ± saÄŸlar.
    # Not: main.py'deki PROJECT_ID'nin deÄŸeri burada geÃ§ersiz olacaktÄ±r.
    PROJECT_ID = st.secrets["GOOGLE_PROJECT_ID"]

    st.session_state["location"] = "us-central1" # Streamlit baÅŸlÄ±ÄŸÄ± iÃ§in konumu sakla
except Exception:
    # Secrets ayarlanmamÄ±ÅŸsa, lokalde Ã§alÄ±ÅŸmayÄ± denemek iÃ§in hata vermeden geÃ§
    pass 
# ---------------------------------------------------------------------------------


# Streamlit, bu fonksiyonu sadece bir kere Ã§alÄ±ÅŸtÄ±rÄ±r ve sonucunu Ã¶nbelleÄŸe alÄ±r.
# app.py dosyasÄ±ndaki setup_rag_pipeline fonksiyonu

# app.py dosyasÄ±ndaki setup_rag_pipeline fonksiyonu, bu hali almalÄ±:

# app.py dosyasÄ±ndaki setup_rag_pipeline fonksiyonu

@st.cache_resource
def setup_rag_pipeline():
    """RAG zincirini yÃ¼kler ve hazÄ±rlar, yoksa oluÅŸturur."""
    
    if not PROJECT_ID:
        st.error("HATA: Proje ID'si ayarlanmadÄ±.")
        return None

    # 1. VektÃ¶r deposu yÃ¼kleniyor
    st.write("VektÃ¶r deposu yÃ¼kleniyor...")
    vector_store = load_vector_store(project_id=PROJECT_ID)
    
    # 2. EÄŸer yÃ¼klenemezse (klasÃ¶r yoksa), sÄ±fÄ±rdan oluÅŸturmayÄ± dene (OTOMATÄ°K OLUÅTURMA)
    if not vector_store:
        # SarÄ± uyarÄ±yÄ± gÃ¶sterir (FAISS'in oluÅŸturulduÄŸu an)
        st.warning("FAISS klasÃ¶rÃ¼ bulunamadÄ±. VeritabanÄ± yeniden oluÅŸturuluyor. Bu iÅŸlem birkaÃ§ dakika sÃ¼rebilir...")
        
        # Veri setini yÃ¼kle
        recipe_docs = load_and_prepare_data('recipes.csv') 
        
        if recipe_docs:
            # VektÃ¶r veritabanÄ±nÄ± oluÅŸtur ve kaydet (API Ã§aÄŸrÄ±sÄ± burada gerÃ§ekleÅŸir!)
            create_new_vector_store(recipe_docs, PROJECT_ID)
            
            # OluÅŸturulduktan sonra tekrar yÃ¼klemeyi dene
            vector_store = load_vector_store(project_id=PROJECT_ID)
            
            if not vector_store:
                 # VeritabanÄ± oluÅŸturma baÅŸarÄ±lÄ± olmasÄ±na raÄŸmen yÃ¼klenemezse bu hatayÄ± verir.
                 st.error("VeritabanÄ± oluÅŸturulduktan sonra bile yÃ¼klenemedi. LÃ¼tfen loglarÄ± kontrol edin.")
                 return None
        else:
             # recipes.csv bulunamazsa bu hatayÄ± verir.
             st.error("Veri (recipes.csv) yÃ¼klenemediÄŸi iÃ§in veritabanÄ± oluÅŸturulamadÄ±.")
             return None

    # 3. Sohbet zinciri oluÅŸturuluyor
    st.write("Sohbet zinciri oluÅŸturuluyor...")
    chain = create_conversational_chain(project_id=PROJECT_ID, vector_store=vector_store)
    
    if not chain:
        st.error("Sohbet zinciri oluÅŸturulurken bir hata oluÅŸtu.")
        return None
        
    return chain
# --- Streamlit ArayÃ¼zÃ¼ ---

st.set_page_config(page_title="Tarif AsistanÄ±", layout="wide")
st.title("ğŸœ SaÄŸlÄ±klÄ± Tarif AsistanÄ± (RAG Chatbot)")
# BaÅŸlÄ±kta location bilgisini de gÃ¶sterelim
st.caption(f"LLM: Gemini / Embedding: text-embedding-004 / Konum: {st.session_state.get('location', 'us-central1')}")

# RAG zincirini kur
rag_chain = setup_rag_pipeline()

if rag_chain:
    
    # Sohbet geÃ§miÅŸini Streamlit session state'te tut
    if "messages" not in st.session_state:
        st.session_state.messages = []
        # Ä°lk karÅŸÄ±lama mesajÄ±
        st.session_state.messages.append({"role": "assistant", "content": "Merhaba! Ben sizin tarif asistanÄ±nÄ±zÄ±m. NasÄ±l bir tarif arÄ±yorsunuz?"})

    # GeÃ§miÅŸ mesajlarÄ± gÃ¶rÃ¼ntÃ¼le
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # KullanÄ±cÄ±dan girdi al
    if prompt := st.chat_input("Tarif sorunuzu buraya yazÄ±n..."):
        
        # KullanÄ±cÄ± mesajÄ±nÄ± geÃ§miÅŸe ekle ve gÃ¶ster
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # AsistanÄ±n cevabÄ±nÄ± al
        with st.chat_message("assistant"):
            with st.spinner("Tarif aranÄ±yor..."):
                try:
                    # LangChain zincirini Ã§aÄŸÄ±rÄ±yoruz
                    response = rag_chain.invoke({'question': prompt})
                    full_answer = response['answer']
                except Exception as e:
                    # Hata olursa logla ve kullanÄ±cÄ±ya gÃ¶ster
                    print(f"Zincir Ã§aÄŸrÄ±lÄ±rken hata: {e}")
                    full_answer = "ÃœzgÃ¼nÃ¼m, API Ã§aÄŸrÄ±sÄ±nda bir sorun oluÅŸtu. LÃ¼tfen uygulamanÄ±n loglarÄ±nÄ± kontrol edin."
                
                st.markdown(full_answer)
                
        # Asistan mesajÄ±nÄ± geÃ§miÅŸe ekle
        st.session_state.messages.append({"role": "assistant", "content": full_answer})