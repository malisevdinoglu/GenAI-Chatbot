import streamlit as st
import os

# main.py dosyasÄ±ndan gerekli fonksiyonlarÄ± ve deÄŸiÅŸkeni import ediyoruz
# Not: main.py dosyanÄ±zÄ±n aynÄ± klasÃ¶rde olduÄŸundan emin olun!
from main import load_vector_store, create_conversational_chain, PROJECT_ID 

# Streamlit, bu fonksiyonu sadece bir kere Ã§alÄ±ÅŸtÄ±rÄ±r ve sonucunu Ã¶nbelleÄŸe alÄ±r.
@st.cache_resource
def setup_rag_pipeline():
    """RAG zincirini yÃ¼kler ve hazÄ±rlar."""
    
    # Proje ID'sinin ayarlÄ± olduÄŸundan emin olalÄ±m (main.py dosyanÄ±zdan geliyor)
    if not PROJECT_ID or PROJECT_ID == "SENIN-PROJE-IDN":
        st.error("HATA: LÃ¼tfen main.py dosyasÄ±nda geÃ§erli bir Proje ID'si girin.")
        return None

    # VektÃ¶r deposu yÃ¼kleniyor
    st.write("VektÃ¶r deposu yÃ¼kleniyor...")
    vector_store = load_vector_store(project_id=PROJECT_ID)
    if not vector_store:
        st.error("VektÃ¶r deposu yÃ¼klenemedi. LÃ¼tfen FAISS klasÃ¶rÃ¼nÃ¼n varlÄ±ÄŸÄ±nÄ± ve main.py'deki konumunu kontrol edin.")
        return None
    
    # Sohbet zinciri oluÅŸturuluyor
    st.write("Sohbet zinciri oluÅŸturuluyor...")
    chain = create_conversational_chain(project_id=PROJECT_ID, vector_store=vector_store)
    
    if not chain:
        st.error("Sohbet zinciri oluÅŸturulurken bir hata oluÅŸtu.")
        return None
        
    return chain

# --- Streamlit ArayÃ¼zÃ¼ ---

st.set_page_config(page_title="Tarif AsistanÄ±", layout="wide")
st.title("ğŸœ SaÄŸlÄ±klÄ± Tarif AsistanÄ± (RAG Chatbot)")
st.caption(f"LLM: Gemini / Embedding: text-embedding-004 / Konum: {st.session_state.get('location', 'us-central1')}")

# RAG zincirini kur
rag_chain = setup_rag_pipeline()

if rag_chain:
    
    # Sohbet geÃ§miÅŸini Streamlit session state'te tut
    if "messages" not in st.session_state:
        st.session_state.messages = []
        # Ä°lk karÅŸÄ±lama mesajÄ±
        st.session_state.messages.append({"role": "assistant", "content": "Merhaba! Ben sizin tarif asistanÄ±nÄ±zÄ±m. NasÄ±l bir tarif arÄ±yorsunuz?"})

    # GeÃ§miÅŸ mesajlarÄ± gÃ¶rÃ¼ntÃ¼le
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # KullanÄ±cÄ±dan girdi al
    if prompt := st.chat_input("Tarif sorunuzu buraya yazÄ±n..."):
        
        # KullanÄ±cÄ± mesajÄ±nÄ± geÃ§miÅŸe ekle ve gÃ¶ster
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # AsistanÄ±n cevabÄ±nÄ± al
        with st.chat_message("assistant"):
            with st.spinner("Tarif aranÄ±yor..."):
                try:
                    # LangChain zincirini Ã§aÄŸÄ±rÄ±yoruz
                    response = rag_chain.invoke({'question': prompt})
                    full_answer = response['answer']
                except Exception as e:
                    # Hata olursa logla ve kullanÄ±cÄ±ya gÃ¶ster
                    print(f"Zincir Ã§aÄŸrÄ±lÄ±rken hata: {e}")
                    full_answer = "ÃœzgÃ¼nÃ¼m, API Ã§aÄŸrÄ±sÄ±nda bir sorun oluÅŸtu. LÃ¼tfen terminali kontrol edin."
                
                st.markdown(full_answer)
                
        # Asistan mesajÄ±nÄ± geÃ§miÅŸe ekle
        st.session_state.messages.append({"role": "assistant", "content": full_answer})